Over the past 80 years, most of algorithmic development has relied on the concept of {\em correct computing}, in particular at the level of the basic compute unit: the {\em arithmetic logic unit (ALU)}. All operations that change the state of static variables --- either stored in registers or random-access memory (RAM) in a computer --- rely on this special-purpose circuit. This digital circuit performs arithmetic and bitwise operations on binary numbers; it is the fundamental building block of the central processing unit (CPU) of computers, floating-point units (FPUs), and even graphics processing units (GPUs) \cite{HamVraZak2012i}. Note that an ALU is not only used for numerical computations on program state variables but also essential for implementing the non-linear process flow in a CPU by virtue of using it to compute the next location in RAM where the program execution continues through arithmetic operations on the so called {\em program counter} (register). 

The concept of correct computing has led to incredible advancements in the field of computational architecture as well as operating systems and databases. However, with the rapid rise of machine learning (ML) and artificial intelligence (AI) systems over the past 20 years, correctness in computing is not as essential as in operating systems or databases; almost all ML and AI algorithm are based on numerical approximations of (cost) minimization problems (see, e.g., \cite{Mit1997a,Bis2006i,Mur2012q}) and current research into energy-efficient ML/AI already focuses on {\em approximate representations} of numbers (e.g., $k$-bit deep neural networks \cite{CouBenDav2015y}). In this paper, we will explore an orthogonal approach where rather than assuming that the representation of parameters is approximate, we assume that we reduce the voltage of a processing unit below the critical threshold where computational errors can be excluded. It is known that the switching power consumption of a CMOS transistor is directly proportional to the frequency $f$ and also proportional to the quadratic on the supply voltage \cite[Subsection 4.1.2]{St2014a}. However, when lowering voltage below a critical threshold \cite{OnuYukSal2020g}, bit flip errors will occur.

The concept we are studying are closely related to the idea of {\em approximate computing} (see \cite{XuMytKim2022a} for an overview). In approximate computing, it will be assumed that both hardware and software are only providing {\em estimates} of the desired behavior in exchange for energy efficiency. In this work, we will pursue this direction by  studying an ALU under an assumed noise level where bit flips occur as part of the {\em input} to a transistor (and hence the logical gate units). In Section \ref{sec:adders}, we will start with describing the most basic building block of a (bitwise) {\em half-adder} before progressing to describe a (bitwise) {\em full-adder} and then a 4-bit ripple-carry adder (RCA). This will be followed by a description of our noise model and the resulting distributions of the outputs of a half-adder, full-adder as well as the 4-bit RCA in Section \ref{sec:noisy_adders}. In Section \ref{sec:redundant_adders}, we will the present our main idea how to reduce the noise in the output of the respective adders and demonstrate some theoretical results. Finally, in Section , we will present empirical results where we mapped our proposed redundancy-adder design onto a field-programmable array (FPGA) and evaluated the real-world distributions resulting from undervolting ALUs.
